{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Next Word Prediction (Beta).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZWrkKXKZidX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "from keras.models import Model\n",
        "#from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Input,BatchNormalization,Dropout\n",
        "from keras.layers import LSTM\n",
        "#from keras.optimizers import Adam,Adadelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from urllib.request import urlopen # the lib that handles the url stuff\n",
        "import string\n",
        "#import pickle\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "#from tensorflow.keras.callbacks import TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2OdRqg0a57I",
        "outputId": "d5711d96-ac9e-47ce-f8fa-cc45d020d958"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRqDsvSva_JO"
      },
      "source": [
        "corpus= 'https://dev.gutenberg.org/files/63677/63677-0.txt' # The Project Gutenberg EBook of The Recluse, by Mike Curry\n",
        "corpus2= 'https://dev.gutenberg.org/files/63964/63964-0.txt' # TThe Project Gutenberg EBook of The Convict Ship, Volume 1 (of 3), by William Clark Russell\n",
        "\n",
        "# Data Cleaning\n",
        "\n",
        "data = urlopen(corpus,) # it's a file like object and works just like a file\n",
        "clean_data=[x.decode('utf-8').strip() for x in data]\n",
        "\n",
        "data2 = urlopen(corpus2,) # it's a file like object and works just like a file\n",
        "clean_data2=[x.decode('utf-8').strip() for x in data2]\n",
        "clean_data2=clean_data2[166:] #remove the early description in the text.\n",
        "combined_data= clean_data+clean_data2\n",
        "\n",
        "combined_data=list(filter(None, combined_data))\n",
        "cleaned=' '.join(combined_data)\n",
        "cleaned=cleaned.translate(str.maketrans('', '', string.punctuation)) #remve punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8vR6VSGbTIX"
      },
      "source": [
        "tokens = word_tokenize(cleaned)\n",
        "train_len = 4\n",
        "text_sequences = []\n",
        "for i in range(train_len,len(tokens)):\n",
        "  seq = tokens[i-train_len:i]\n",
        "  text_sequences.append(seq)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "#vocabulary size increased by 1 for the cause of padding\n",
        "vocabulary_size = len(tokenizer.word_counts)+1\n",
        "n_sequences = np.empty([len(sequences),train_len], dtype='float64')\n",
        "for i in range(len(sequences)):\n",
        "  n_sequences[i] = sequences[i]\n",
        "train_inputs = n_sequences[:,:-1]\n",
        "train_targets = n_sequences[:,-1]\n",
        "#train_targets = to_categorical(train_targets, num_classes=vocabulary_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGJ33tWwhH3i"
      },
      "source": [
        "#text_sequences\n",
        "train_texts=[]\n",
        "target_texts=[]\n",
        "for i in text_sequences:\n",
        "  train_texts.append(i[:-1]) \n",
        "  target_texts.append(i[-1])\n",
        "\n",
        "train_txt=[]\n",
        "for text in train_texts:  \n",
        "  my_lst_str = [' '.join(map(str,text))]\n",
        "  train_txt.append(my_lst_str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOtywiwnhyae"
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
        "\n",
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.load(module_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqmapyO6oHsK"
      },
      "source": [
        "train_text=[sentence[0] for sentence in train_txt]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFBGADi2oJdK",
        "outputId": "18f28550-deab-41b1-9a09-ef80627ddab6"
      },
      "source": [
        "len(train_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67724"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmSMOl1jjB2G"
      },
      "source": [
        "embeddings_train = embed(train_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPnBWviLiZcC"
      },
      "source": [
        "y_train=pd.get_dummies(train_targets).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6-HI_aIjSKG"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(256, activation='relu',input_shape=(512,)))\n",
        "model.add(tf.keras.layers.Reshape((128, 2)))\n",
        "model.add( Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "model.add( Dense(y_train.shape[1],activation='softmax',name='output'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBpqB1sN16p4",
        "outputId": "f3bba8ba-d6ba-46ff-9e97-a84412b64ee7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 128, 2)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128, 2)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128, 2)            8         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               67072     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 6393)              415545    \n",
            "=================================================================\n",
            "Total params: 622,465\n",
            "Trainable params: 622,333\n",
            "Non-trainable params: 132\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGVgU3W51-ez",
        "outputId": "0288f335-37ca-4abc-b827-69549bfee952"
      },
      "source": [
        "# clean up as much as possible\n",
        "import gc\n",
        "print(gc.collect())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6OQOAuk72Yt",
        "outputId": "29ce73cf-c397-4c69-a8ec-def381a96f68"
      },
      "source": [
        "model.compile(optimizer='Adam', loss='CategoricalCrossentropy')\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "model.fit(embeddings_train ,y_train,validation_split=0.2, epochs=20,batch_size=64,callbacks=[callback])\n",
        "\n",
        "model.save('pred_model4.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "847/847 [==============================] - 184s 215ms/step - loss: 7.5307 - val_loss: 6.5248\n",
            "Epoch 2/20\n",
            "847/847 [==============================] - 176s 208ms/step - loss: 6.1922 - val_loss: 6.5475\n",
            "Epoch 3/20\n",
            "847/847 [==============================] - 176s 207ms/step - loss: 6.0902 - val_loss: 6.4597\n",
            "Epoch 4/20\n",
            "847/847 [==============================] - 178s 210ms/step - loss: 6.0261 - val_loss: 6.4100\n",
            "Epoch 5/20\n",
            "847/847 [==============================] - 180s 213ms/step - loss: 5.9757 - val_loss: 6.3709\n",
            "Epoch 6/20\n",
            "847/847 [==============================] - 172s 203ms/step - loss: 5.9000 - val_loss: 6.3770\n",
            "Epoch 7/20\n",
            "847/847 [==============================] - 177s 208ms/step - loss: 5.8751 - val_loss: 6.3507\n",
            "Epoch 8/20\n",
            "847/847 [==============================] - 176s 208ms/step - loss: 5.8048 - val_loss: 6.4092\n",
            "Epoch 9/20\n",
            "847/847 [==============================] - 176s 207ms/step - loss: 5.7754 - val_loss: 6.3968\n",
            "Epoch 10/20\n",
            "847/847 [==============================] - 174s 206ms/step - loss: 5.7560 - val_loss: 6.4321\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}